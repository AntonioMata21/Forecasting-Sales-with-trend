{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":4780.981579,"end_time":"2024-01-11T07:20:46.167602","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-11T06:01:05.186023","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.593318,"end_time":"2024-01-11T06:01:12.457860","exception":false,"start_time":"2024-01-11T06:01:09.864542","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ndatax=df.sample(n=300, random_state=123)\ndatax.set_index('id',inplace=True)\n\ndatax=datax.iloc[:,-1941:]","metadata":{"papermill":{"duration":8.542676,"end_time":"2024-01-11T06:01:21.008881","exception":false,"start_time":"2024-01-11T06:01:12.466205","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef compare_lists(list1, list2):\n    \"\"\"\n    Compara dos listas de diferentes tamaños y cuenta la tendencia de la lista mayor.\n    Retorna \"tendencia al alza\" si list1 tiene más valores mayores que list2,\n    \"tendencia a la baja\" si list2 tiene más valores mayores que list1,\n    y \"empate\" si ambos tienen la misma cantidad de valores mayores.\n    \"\"\"\n    list1 = list1.values.tolist()\n    list2 = list2.values.tolist()\n    min_len = min(len(list1), len(list2))    \n    # Reducir las listas al tamaño del menor\n    list1 = list1[:min_len]   \n    list2 = list2[:min_len]    \n    # Buscar el primer índice donde ambas listas tengan valores no nulos\n    i = 0\n    while i < min_len and (np.isnan(list1[i]) or np.isnan(list2[i])):\n        i += 1\n        \n    # Comparar valores de ambas listas a partir del índice encontrado\n    count_a = 0\n    count_b = 0\n    for j in range(i, min_len):        \n        if list1[j] > list2[j]:\n            count_a += 1\n        elif list2[j] > list1[j]:\n            count_b += 1\n    dif=abs(count_a-count_b)\n    umbral = 3\n    if dif >= umbral:\n        if count_a > count_b:\n            return [\"tendencia al alza\", count_a ,count_b]\n        \n        elif count_b > count_a:\n            return [\"tendencia a la baja\", count_a ,count_b]\n    else:\n        return [\"no tiene tendencia\", count_a, count_b]\n    \ndef plot_moving_averages(data, window1, window2):\n    \"\"\"\n    Calcula y grafica dos promedios móviles de una serie de tiempo.\n    data: la serie de tiempo a procesar.\n    window1: el tamaño de la ventana del primer promedio móvil.\n    window2: el tamaño de la ventana del segundo promedio móvil.\n    \"\"\"\n    #plt.figure(figsize=(16,8))\n    xlabel='Horizonte de Tiempo'\n    ylabel='Valor de la serie'\n    # Calcula los dos promedios móviles\n    data=pd.DataFrame(data)\n    ma1 = data.rolling(window=window1).mean()\n    ma2 = data.rolling(window=window2).mean()\n    \n    new_x_values = np.arange(len(data))\n    \n    # Grafica la serie de tiempo y los dos promedios móviles\n    \n    plt.plot(new_x_values, data, label='Original')\n    plt.plot(new_x_values, ma1,color=\"green\", label=f'MA({window1})')\n    plt.plot(new_x_values, ma2,color=\"red\", label=f'MA({window2})')\n    \n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    \n    \n    plt.legend()\n    plt.show()\n    \n    return ma1,ma2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rax=[]\ndatax2=datax.iloc[:,-30:]\nfor i in range(300):\n    m1,m2=plot_moving_averages(datax2.iloc[i],3,5)\n    rax.append([compare_lists(m1, m2), datax2.index[i]])\n    \n    print(rax[i])\n    \nnrax=[]\nfor i in range(len(rax)):\n    if(rax[i][0][0]!='no tiene tendencia'):\n        nrax.append(rax[i]) \npt=(len(nrax)/len(rax))*100\nprint(f\"{len(nrax)} series fuenron clasificadas con tendencia es decir {pt}%.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\ndef model_lstm(look_back):\n    model=Sequential()\n    model.add(LSTM(100, input_shape=(1, look_back), activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])\n    return model\ndef model_loss(history):\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Test Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='upper right')\n    plt.show();\n\ndef convert2matrix(data_arr, look_back):\n   X, Y =[], []\n   for i in range(len(data_arr)-look_back):\n    d=i+look_back  \n    X.append(data_arr[i:d,])\n    Y.append(data_arr[d,])\n   return np.array(X), np.array(Y)\n\nimport seaborn as sns\n\ndef prediction_plot(testY, test_predict):\n      len_prediction=[x for x in range(len(testY))]\n      plt.figure(figsize=(8,4))\n      plt.plot(len_prediction, testY[:len(testY)], marker='.', label=\"Real\")\n      plt.plot(len_prediction, test_predict[:len(testY)], 'r', label=\"Pronostico\")\n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('Pronostico Tendencia', size=15)\n      plt.xlabel('Tamaño de la serie', size=15)\n      plt.legend(fontsize=15)\n      plt.show();\n\ndef smape(a, f):\n    a = np.array([float(val) for val in a])\n    f = np.array([float(val) for val in f])\n    \n    numerator = 2 * np.abs(f - a)\n    denominator = np.abs(a) + np.abs(f)\n    \n    return 100 / len(a) * np.sum(numerator / denominator)","metadata":{"papermill":{"duration":10.571735,"end_time":"2024-01-11T06:01:38.912513","exception":false,"start_time":"2024-01-11T06:01:28.340778","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smapeList=[]\npredsList=[]\nfor i in range(len(nrax)):\n    \n    #MA=datax.loc[nrax[i][1]].rolling(window=5).mean().dropna()\n    nMA=datax.loc[nrax[i][1]].values\n    data = nMA\n    df_arr=data\n\n    df_arr = np.reshape(df_arr, (-1, 1)) #LTSM requires more input features compared to RNN or DNN\n    scaler = MinMaxScaler(feature_range=(0, 1))#LTSM is senstive to the scale of features\n    df_arr = scaler.fit_transform(df_arr)\n\n    train,test = train_test_split(df_arr,test_size=58,shuffle=False)\n    look_back = 30\n\n    trainX, trainY = convert2matrix(train, look_back)\n    testX, testY = convert2matrix(test, look_back)\n    # reshape input to be [samples, time steps, features]\n    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n    model=model_lstm(look_back)\n    EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n    history=model.fit(trainX,trainY, epochs=100, batch_size=30, verbose=1, validation_data=(testX,testY),callbacks=[EarlyStopping],shuffle=False)\n\n    \n    test_predict = model.predict(testX)\n    # invert predictions\n    \n    test_predict = scaler.inverse_transform(test_predict)\n    testY = scaler.inverse_transform(testY.reshape(-1,1))\n    \n    predsList.append(test_predict)\n    smapeList.append(smape(testY, test_predict))\n    print('-'*20,i,'-'*20)\n    prediction_plot(testY, test_predict)\n    \n","metadata":{"papermill":{"duration":4681.484842,"end_time":"2024-01-11T07:19:40.496727","exception":false,"start_time":"2024-01-11T06:01:39.011885","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=[]\nfor i in range(130):\n    flat_preds = predsList[i].flatten()\n    preds.append(flat_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear un DataFrame\nids=[i[1] for i in nrax]\n\ndf = pd.DataFrame(preds)\ndf['id'] = ids\ndf.set_index('id', inplace=True)\n# Guardar el DataFrame en un archivo CSV\ndf.to_csv('preds_lstm_130sOrginal.csv', index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p= sum(smapeList)/len(smapeList)\nsp=round(p,2)\nprint('sMape promedio: ',sp)","metadata":{"papermill":{"duration":8.976562,"end_time":"2024-01-11T07:19:58.342128","exception":false,"start_time":"2024-01-11T07:19:49.365566","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfr= pd.DataFrame(smapeList)\ndfr.to_csv(\"Results_lstm_130sMA5.csv\")","metadata":{"papermill":{"duration":8.981758,"end_time":"2024-01-11T07:20:16.417396","exception":false,"start_time":"2024-01-11T07:20:07.435638","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}