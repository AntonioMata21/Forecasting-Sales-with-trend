{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ndatax=df.sample(n=300, random_state=123)\ndatax.set_index('id',inplace=True)\n\ndatax=datax.iloc[:,-1941:]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\ndef model_lstm(look_back):\n    model=Sequential()\n    model.add(LSTM(100, input_shape=(1, look_back), activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])\n    return model\ndef model_loss(history):\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Test Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='upper right')\n    plt.show();\n\ndef convert2matrix(data_arr, look_back):\n   X, Y =[], []\n   for i in range(len(data_arr)-look_back):\n    d=i+look_back  \n    X.append(data_arr[i:d,])\n    Y.append(data_arr[d,])\n   return np.array(X), np.array(Y)\n\nimport seaborn as sns\n\ndef prediction_plot(testY, test_predict):\n      len_prediction=[x for x in range(len(testY))]\n      plt.figure(figsize=(8,4))\n      plt.plot(len_prediction, testY[:len(testY)], marker='.', label=\"Real\")\n      plt.plot(len_prediction, test_predict[:len(testY)], 'r', label=\"Pronostico\")\n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('Pronostico Tendencia', size=15)\n      plt.xlabel('Tama√±o de la serie', size=15)\n      plt.legend(fontsize=15)\n      plt.show();\ndef smape(a, f):\n    a = np.array([float(val) for val in a])\n    f = np.array([float(val) for val in f])\n    \n    numerator = 2 * np.abs(f - a)\n    denominator = np.abs(a) + np.abs(f)\n    \n    return 100 / len(a) * np.sum(numerator / denominator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smapeList=[]\npredsList=[]\nfor i in range(300):\n    \n    #MA=datax.iloc[[i][0]].rolling(window=5).mean().dropna()\n    nMA=datax.iloc[[i][0]].values\n    data = nMA\n    df_arr=data\n\n    df_arr = np.reshape(df_arr, (-1, 1)) #LTSM requires more input features compared to RNN or DNN\n    scaler = MinMaxScaler(feature_range=(0, 1))#LTSM is senstive to the scale of features\n    df_arr = scaler.fit_transform(df_arr)\n\n    train,test = train_test_split(df_arr,test_size=58,shuffle=False)\n    look_back = 30\n\n    trainX, trainY = convert2matrix(train, look_back)\n    testX, testY = convert2matrix(test, look_back)\n    # reshape input to be [samples, time steps, features]\n    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n    model=model_lstm(look_back)\n    EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n    history=model.fit(trainX,trainY, epochs=100, batch_size=30, verbose=1, validation_data=(testX,testY),callbacks=[EarlyStopping],shuffle=False)\n\n    \n    test_predict = model.predict(testX)\n    # invert predictions\n   \n    test_predict = scaler.inverse_transform(test_predict)\n    testY = scaler.inverse_transform(testY.reshape(-1,1))\n    \n    predsList.append(test_predict)\n    smapeList.append(smape(testY, test_predict))\n    print('-'*20,i,'-'*20)\n    prediction_plot(testY, test_predict)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=[]\nfor i in range(300):\n    flat_preds = predsList[i].flatten()\n    preds.append(flat_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear un DataFrame\nidsx=datax.index.tolist()\nids=idsx\n\ndf = pd.DataFrame(preds)\ndf['id'] = ids\ndf.set_index('id', inplace=True)\n# Guardar el DataFrame en un archivo CSV\ndf.to_csv('preds_lstm_300sOrginal.csv', index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p= sum(smapeList)/len(smapeList)\nsp=round(p,2)\nprint('sMape promedio: ',sp)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T17:41:52.801092Z","iopub.execute_input":"2024-02-22T17:41:52.801513Z","iopub.status.idle":"2024-02-22T17:41:52.807297Z","shell.execute_reply.started":"2024-02-22T17:41:52.801482Z","shell.execute_reply":"2024-02-22T17:41:52.806422Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"sMape promedio:  138.7\n","output_type":"stream"}]},{"cell_type":"code","source":"dfr= pd.DataFrame(smapeList)\ndfr.to_csv(\"Results_lstm_300s.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}